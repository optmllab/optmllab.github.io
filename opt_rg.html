---
layout: default
title: Home
---
<div class="wrapper row0">
<h1>Optimization Reading Group</h1>

<p><b>The goal of the seminar</b> is to read papers on the selected topic, generate new ideas and solutions.</p>

<p><b>The main rule of the seminar:</b> it is forbiden to participate without reading the paper before the seminar. We introduce this rule
to focus more on the specific details and generating new ideas during the seminars. Besides this rule, we do not impose any specific
rules going beyond adequate scientific behavior.</p>

<h2>Spring 2024: Parameter-Free Methods</h2>

<p>In this series of meetings, we will focus on the parameter-free optimization methods.</p>

<h3>Future Seminars</h3>

<ol type="1">
    <li>February 9, 2024 (Class Room 3, 15:00 - 17:00 GMT+4). Discussion of the paper <a href="https://arxiv.org/abs/2310.12139">"Optimal and parameter-free gradient minimization methods for convex and nonconvex optimization"</a> (Guanghui Lan, Yuyuan Ouyang, Zhe Zhang)</li>
    <li>February 16, 2024. No meeting.</li>
    <li>February 23, 2024. No meeting.</li>
    <li>March 1, 2024 (Class Room 3, 15:00 - 17:00 GMT+4). Discussion of the papers <a href="https://arxiv.org/abs/2301.07733">"Learning-Rate-Free Learning by D-Adaptation"</a> (Aaron Defazio, Konstantin Mishchenko), <a href="https://arxiv.org/abs/2306.06101">"Prodigy: An Expeditiously Adaptive Parameter-Free Learner"</a> (Konstantin Mishchenko, Aaron Defazio), and <a href="https://arxiv.org/abs/2402.07793">"Tuning-Free Stochastic Optimization"</a> (Ahmed Khaled, Chi Jin)</li>
  </ol>  
</div>